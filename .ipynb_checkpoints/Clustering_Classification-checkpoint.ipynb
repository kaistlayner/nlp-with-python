{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHOLE PIPELINE\n",
    "\n",
    "1. Feature vector 들을 받아온다.\n",
    "***\n",
    "2. Feature vector 들을 Normalize 한다.  \n",
    "    + 0 ~ 1 사이의 값으로 Normalize\n",
    "    + Normalize factor를 같이 저장한다.\n",
    "***\n",
    "2. Feature vector 들을 Clustering 한다.  \n",
    "    + K-MEANS 알고리즘을 이용해서 군집화\n",
    "***\n",
    "3. Clustering 된 Centroids 를 이용해서 Training Dataset 을 만든다.  \n",
    "    + Centroids의 Index들을 데이터의 Training Label 로써 이용한다.  \n",
    "    + 이때 만들어진 데이터를 csv format을 이용해서 저장한다.  \n",
    "***\n",
    "4. Traning Data를 필요에 따라서 Training Data와 Validation Data 로 구분한다.\n",
    "    + 현재 별도의 Validation은 진행하지 않을 예정\n",
    "***\n",
    "5. Trainor 를 통해서 Classifying Model을 학습시킨다.  \n",
    "    + Trainor 내에서는 Training data를  불러오고 카테고리 개수의 SVM을 학습시킨다.  \n",
    "    + 이때 Trainior 는 SVM 파라미터를 입력으로 받는다.\n",
    "    + https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    + Train Accuracy 를 파악한다.  \n",
    "    + 모델은 필요한 곳에 저장하도록 한다.\n",
    "***\n",
    "6. Predict 는 임의의 Feature에 대해서 해당 카테고리를 return 하도록 한다.\n",
    "    + Predict를 통해서 해당 centroid 의 값들을 통해서 어떤 특징을 가지고 있는지도 파악하도록 한다\n",
    "    \n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy\n",
    "import scv\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(des):\n",
    "    \"\"\"\n",
    "    Normalize the features all values from -1 to 1\n",
    "    \n",
    "    :param des: Length f features from n unique actors\n",
    "    :type des: numpy.ndarray. shape: [num_images, dim_feature]\n",
    "    return: Normalized features\n",
    "    rtype: numpy.ndarray. shape: [num_images, dim_feature]\n",
    "    \"\"\"\n",
    "    \n",
    "    des = des - ((np.max(a, axis=0) + np.min(a, axis=0)) / 2)\n",
    "    des = des / ((np.max(a, axis=0) - np.min(a, axis=0)) / 2)\n",
    "    \n",
    "    return des\n",
    "\n",
    "def get_cluster(des, k, thres):\n",
    "    \"\"\"\n",
    "    Make clusters from given features\n",
    "    \n",
    "    :param des: length f features from n unique actors\n",
    "    :type des: numpy.ndarray. shape: [num_images, dim_feature]\n",
    "    :param k: number of clusters \n",
    "    :type k: int\n",
    "    :param thres: threshold for K-MEAN clustering algorithm\n",
    "    :type thres: int\n",
    "    :return: k centroids with f features\n",
    "    :rtype: numpy.ndarray. shape: [num_centorids, dim_feature]\n",
    "    \"\"\"\n",
    "    \n",
    "    des = normalizer(des)\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    f = np.shape(des)[1]\n",
    "\n",
    "    centroids = np.array([np.random.rand(f) for i in range(k)])\n",
    "    \n",
    "    while(1):\n",
    "        prev_centroids = centroids\n",
    "        centroids = np.zeros((k, f))\n",
    "        data_length = [0] * k\n",
    "\n",
    "        for feat in des:\n",
    "            idx = 0\n",
    "            min = -1\n",
    "\n",
    "            for cent, i in zip(prev_centroids, range(len(prev_centroids))):\n",
    "                if (np.linalg.norm(feat-cent) < min or min == -1):\n",
    "                    idx = i\n",
    "                    min = np.linalg.norm(feat-cent)\n",
    "\n",
    "            data_length[idx] += 1\n",
    "            centroids[idx] += feat\n",
    "\n",
    "        for i in range(len(centroids)):\n",
    "            if (data_length[i] != 0):\n",
    "                centroids[i] = centroids[i] / data_length[i]\n",
    "            else:\n",
    "                centroids[i] = np.random.rand(f)\n",
    "\n",
    "    if (np.linalg.norm(centroids - prev_centroids) < thres):\n",
    "        break\n",
    "\n",
    "    return centroids\n",
    "\n",
    "def make_train_set(des, cent):\n",
    "    \"\"\"\n",
    "    Make training dataset from centroids and descriptors\n",
    "    \n",
    "    :param des: length f features from n unique actors\n",
    "    :type des: numpy.ndarray. shape: [num_images, dim_feature]\n",
    "    :param cent: list of k centroids\n",
    "    :type cent: numpy.ndarray. shape: [num_centorids, dim_feature]\n",
    "    :return: None\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    \n",
    "    label = []\n",
    "    \n",
    "    for feat in des:\n",
    "        label.append(np.argmin(np.linalg.norm(cent - feat, axis = 1)))\n",
    "    \n",
    "    if not os.path.isdir(\"train_data\"):\n",
    "        os.mkdir(\"train_data\")\n",
    "    \n",
    "    f = open('train_data/train_set.csv','w', newline='')\n",
    "    wr = csv.writer(f)\n",
    "    \n",
    "    for i, feat in enumerate(des):\n",
    "        wr.writerow(list(feat) + [label[i]])\n",
    "        \n",
    "    return None\n",
    "\n",
    "def load_train_data(data_dir):\n",
    "    \"\"\"\n",
    "    Load training dataset from given data directory\n",
    "    \n",
    "    :param data_dir: data directory to load the training data\n",
    "    :type data_dir: string\n",
    "    :return: train images and train labels\n",
    "    :rtype: Tuple of lists\n",
    "    List of numpy.ndarray. shape: [num_images, dim_feature] \n",
    "    List of numpy.ndarray. shape: [num_images, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "def train_classifier(features, labels, svm_params):\n",
    "    \"\"\"\n",
    "    Train the SVM classifier\n",
    "\n",
    "    :param features: Features to train\n",
    "    :type features: numpy.ndarray. shape: [num_images, dim_feature]\n",
    "    :param labels: Labels to train\n",
    "    :type labels: numpy.ndarray. shape: [num_images, 1]\n",
    "    :param svm_params: SVM parameters\n",
    "    :type svm_params: dict\n",
    "    ['C'](float): Regularization parameter\n",
    "    ['kernel'](str): Kernel specification\n",
    "    :return: Trained classifier\n",
    "    :rtype: sklearn.svm.SVC\n",
    "    \"\"\"\n",
    "\n",
    "    svm = SVC(kernel=svm_params['kernel'], C=svm_params['C'], random_state=0)\n",
    "    svm.fit(features, labels)\n",
    "\n",
    "    return svm\n",
    "\n",
    "def Trainer(feat_param, svm_param):\n",
    "    \"\"\"\n",
    "    Train SVM models to classify new feature\n",
    "      \n",
    "    :param svm_params: SVM parameters\n",
    "    :type svm_params: dict\n",
    "    ['C'](float): Regularization parameter\n",
    "    ['kernel'](str): Kernel specification\n",
    "    :return: Trained classifier\n",
    "    :rtype: sklearn.svm.SVC\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.isdir(\"model\"):\n",
    "        os.mkdir(\"model\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_des, train_idxs = load_train_data(\"train_data/train_set.csv\")\n",
    "\n",
    "    print('Train the classifiers...')\n",
    "    accuracy = 0\n",
    "    models = {}\n",
    "    \n",
    "    for class_name in category:\n",
    "        target_idxs = np.array([read_txt(os.path.join(data_dir, '{}_train.txt'.format(class_name)))])\n",
    "        target_labels = get_labels(train_idxs, target_idxs)\n",
    "        \n",
    "        models[class_name] = train_classifier(train_features, target_labels, svm_params)\n",
    "        train_accuracy = models[class_name].score(train_features, target_labels) \n",
    "        print('{} Classifier train accuracy:  {:.4f}'.format(class_name ,train_accuracy))\n",
    "        accuracy += train_accuracy\n",
    "    \n",
    "    print('Average train accuracy: {:.4f}'.format(accuracy/len(category)))\n",
    "    del train_features, target_labels, target_idxs\n",
    "\n",
    "    return models    \n",
    "    \n",
    "    return None\n",
    "\n",
    "def Predict(feat, models, cent):\n",
    "    \"\"\"\n",
    "    Predict the label based on the trained SVM classifier\n",
    "    \n",
    "    :param models: Trained model\n",
    "    :type models: sklearn.svm.SVC\n",
    "    :param cent: Centroids\n",
    "    :type cent: numpy.ndarray. shape: [num_centorids, dim_feature]\n",
    "    :return: Correspoding Centroids with label\n",
    "    :rtype: TBD\n",
    "    \"\"\"\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1, 1], [2, 2]])\n",
    "\n",
    "type(a)\n",
    "\n",
    "np.shape(a)\n",
    "\n",
    "a = np.array([[1,2, 3], [4, 1, 0], [-5, 2, -3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2 3]\n",
      "[-5  1 -3]\n",
      "[4.5 0.5 3. ]\n",
      "[-0.5  1.5  0. ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.33333333,  1.        ,  1.        ],\n",
       "       [ 1.        , -1.        ,  0.        ],\n",
       "       [-1.        ,  1.        , -1.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.max(a, axis=0))\n",
    "print(np.min(a, axis=0))\n",
    "print((np.max(a, axis=0) - np.min(a, axis=0))/2)\n",
    "print((np.max(a, axis=0) + np.min(a, axis=0))/2)\n",
    "(a - ((np.max(a, axis=0) + np.min(a, axis=0))/2)) / ((np.max(a, axis=0) - np.min(a, axis=0))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  1  0  0]\n",
      " [-3 -3 -3 -4  5  2  5]\n",
      " [ 0 -6 -1  2  1  3  5]]\n",
      "[1.         9.8488578  8.71779789]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3, 4, 5, 6, 7], [4, 5, 6, 8, 1, 4, 2], [1, 8, 4, 2, 5, 3, 2]])\n",
    "b = np.array([1, 2, 3, 4, 6, 6, 7])\n",
    "\n",
    "print(b-a)\n",
    "\n",
    "print(np.linalg.norm(a - b, axis = 1))\n",
    "\n",
    "print(np.argmin(np.linalg.norm(a - b, axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "f = open('write.csv','w', newline='')\n",
    "wr = csv.writer(f)\n",
    "\n",
    "c = np.array([1, 2, 3])\n",
    "\n",
    "wr.writerow(list(a[0]) + [c[0]])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
