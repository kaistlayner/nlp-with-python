{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHOLE PIPELINE\n",
    "\n",
    "1. Feature vector 들을 받아온다.\n",
    "***\n",
    "2. Feature vector 들을 Normalize 한다.  \n",
    "    + 0 ~ 1 사이의 값으로 Normalize\n",
    "    + Normalize factor를 같이 저장한다.\n",
    "***\n",
    "2. Feature vector 들을 Clustering 한다.  \n",
    "    + K-MEANS 알고리즘을 이용해서 군집화\n",
    "***\n",
    "3. Clustering 된 Centroids 를 이용해서 Training Dataset 을 만든다.  \n",
    "    + Centroids의 Index들을 데이터의 Training Label 로써 이용한다.  \n",
    "    + 이때 만들어진 데이터를 csv format을 이용해서 저장한다.  \n",
    "***\n",
    "4. Traning Data를 필요에 따라서 Training Data와 Validation Data 로 구분한다.\n",
    "    + 현재 별도의 Validation은 진행하지 않을 예정\n",
    "***\n",
    "5. Trainor 를 통해서 Classifying Model을 학습시킨다.  \n",
    "    + Trainor 내에서는 Training data를  불러오고 카테고리 개수의 SVM을 학습시킨다.  \n",
    "    + 이때 Trainior 는 SVM 파라미터를 입력으로 받는다.\n",
    "    + https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    + Train Accuracy 를 파악한다.  \n",
    "    + 모델은 필요한 곳에 저장하도록 한다.\n",
    "***\n",
    "6. Predict 는 임의의 Feature에 대해서 해당 카테고리를 return 하도록 한다.\n",
    "    + Predict를 통해서 해당 centroid 의 값들을 통해서 어떤 특징을 가지고 있는지도 파악하도록 한다\n",
    "    \n",
    "***\n",
    "# Changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy\n",
    "import scv\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(des):\n",
    "    \"\"\"\n",
    "    Normalize the features all values from -1 to 1\n",
    "    \n",
    "    :param des: Length f features from n unique actors\n",
    "    :type des: numpy.ndarray. shape: [num_features, dim_feature]\n",
    "    return: Normalized features\n",
    "    rtype: numpy.ndarray. shape: [num_features, dim_feature]\n",
    "    \"\"\"\n",
    "    \n",
    "    des = des - ((np.max(a, axis=0) + np.min(a, axis=0)) / 2)\n",
    "    des = des / ((np.max(a, axis=0) - np.min(a, axis=0)) / 2)\n",
    "    \n",
    "    return des\n",
    "\n",
    "def get_cluster(des, k, thres):\n",
    "    \"\"\"\n",
    "    Make clusters from given features\n",
    "    \n",
    "    :param des: length f features from n unique actors\n",
    "    :type des: numpy.ndarray. shape: [num_features, dim_feature]\n",
    "    :param k: number of clusters \n",
    "    :type k: int\n",
    "    :param thres: threshold for K-MEAN clustering algorithm\n",
    "    :type thres: int\n",
    "    :return: k centroids with f features\n",
    "    :rtype: numpy.ndarray. shape: [num_centorids, dim_feature]\n",
    "    \"\"\"\n",
    "    \n",
    "    des = normalizer(des)\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    f = np.shape(des)[1]\n",
    "\n",
    "    centroids = np.array([np.random.rand(f) for i in range(k)])\n",
    "    \n",
    "    while(1):\n",
    "        prev_centroids = centroids\n",
    "        centroids = np.zeros((k, f))\n",
    "        data_length = [0] * k\n",
    "\n",
    "        for feat in des:\n",
    "            idx = 0\n",
    "            min = -1\n",
    "\n",
    "            for cent, i in zip(prev_centroids, range(len(prev_centroids))):\n",
    "                if (np.linalg.norm(feat-cent) < min or min == -1):\n",
    "                    idx = i\n",
    "                    min = np.linalg.norm(feat-cent)\n",
    "\n",
    "            data_length[idx] += 1\n",
    "            centroids[idx] += feat\n",
    "\n",
    "        for i in range(len(centroids)):\n",
    "            if (data_length[i] != 0):\n",
    "                centroids[i] = centroids[i] / data_length[i]\n",
    "            else:\n",
    "                centroids[i] = np.random.rand(f)\n",
    "\n",
    "        if (np.linalg.norm(centroids - prev_centroids) < thres):\n",
    "            break\n",
    "\n",
    "    return centroids\n",
    "\n",
    "def get_labels(des, cent):\n",
    "    \"\"\"\n",
    "    Make training dataset from centroids and descriptors\n",
    "    \n",
    "    :param des: length f features from n unique actors\n",
    "    :type des: numpy.ndarray. shape: [num_features, dim_feature]\n",
    "    :param cent: list of k centroids\n",
    "    :type cent: numpy.ndarray. shape: [num_centorids, dim_feature]\n",
    "    :return: Labels\n",
    "    :rtype: numpy.ndarray. shape: [num_features, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for feat in des:\n",
    "        labels.append(np.argmin(np.linalg.norm(cent - feat, axis = 1)))\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def filter_db(db):\n",
    "    \"\"\"\n",
    "    Filter the database based on some criteria\n",
    "    e.g., exclude person who said no more than 5 sentenses throughout the movie\n",
    "    \n",
    "    :param db: Database\n",
    "    :type db: dict\n",
    "    ['name'](str): Name of actor to sentenses one said in the movie\n",
    "    :return: Modified Database\n",
    "    :rtype: dict\n",
    "    ['name'](str): Name of actor to sentenses one said in the movie\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "def extract_des(db):\n",
    "    \"\"\"\n",
    "    Extract the feature for the PREPROCESSING from the database\n",
    "    \n",
    "    :param db: Database\n",
    "    :type db: dict\n",
    "    ['name'](str): Name of actor to sentenses one said in the movie\n",
    "    :return: Modified Database\n",
    "    :rtype: numpy.ndarray. shape: [num_features, dim_feature]\n",
    "    ['name'](str): Name of actor to sentenses one said in the movie    \n",
    "    \"\"\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_feat(feats):\n",
    "    \"\"\"\n",
    "    Extract the feature for the TRAINING DATA SET\n",
    "    \n",
    "    :param feats: List of sentences to extract feature\n",
    "    :type feats: list. size: [num_sentences, 1 (string)]\n",
    "    :return: Extracted feature\n",
    "    :rtype: list. shape: [dim_feature2]\n",
    "    \"\"\"\n",
    "\n",
    "def make_train_set(db, k, thres, num_sents):\n",
    "    \"\"\"\n",
    "    Make training data set from database and features\n",
    "    \n",
    "    :param db: Database\n",
    "    :type db: dict\n",
    "    ['name'](str): Name of actor to sentenses one said in the movie\n",
    "    :param k: Number of clusters for K-MEANS algorithm\n",
    "    :type: int\n",
    "    :param thres: Threshold for K_MEANS algorithm\n",
    "    :type: int\n",
    "    :param num_sents: Number of sentences for one training sample\n",
    "    :type: int\n",
    "    :return: None\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(\"train_data\"):\n",
    "        os.mkdir(\"train_data\")  \n",
    "        \n",
    "    db = filter_db(db)\n",
    "    des = extract_des(db)\n",
    "    \n",
    "    centroids = get_cluster(des, k, thres)\n",
    "    labels = get_labels(des, cent)\n",
    "\n",
    "    f = open('train_data/train_set.csv','w', newline='')\n",
    "    wr = csv.writer(f)    \n",
    "    \n",
    "    for i in db:\n",
    "        sents = db[i]\n",
    "        label = labels[i]\n",
    "        \n",
    "        feature = random.sample(sents, num_sents)\n",
    "        feature = extract_feat(feature)\n",
    "        \n",
    "        wr.writerow(feature + [label])\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1, 1], [2, 2]])\n",
    "\n",
    "type(a)\n",
    "\n",
    "np.shape(a)\n",
    "\n",
    "a = np.array([[1,2, 3], [4, 1, 0], [-5, 2, -3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2 3]\n",
      "[-5  1 -3]\n",
      "[4.5 0.5 3. ]\n",
      "[-0.5  1.5  0. ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.33333333,  1.        ,  1.        ],\n",
       "       [ 1.        , -1.        ,  0.        ],\n",
       "       [-1.        ,  1.        , -1.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.max(a, axis=0))\n",
    "print(np.min(a, axis=0))\n",
    "print((np.max(a, axis=0) - np.min(a, axis=0))/2)\n",
    "print((np.max(a, axis=0) + np.min(a, axis=0))/2)\n",
    "(a - ((np.max(a, axis=0) + np.min(a, axis=0))/2)) / ((np.max(a, axis=0) - np.min(a, axis=0))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  1  0  0]\n",
      " [-3 -3 -3 -4  5  2  5]\n",
      " [ 0 -6 -1  2  1  3  5]]\n",
      "[1.         9.8488578  8.71779789]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3, 4, 5, 6, 7], [4, 5, 6, 8, 1, 4, 2], [1, 8, 4, 2, 5, 3, 2]])\n",
    "b = np.array([1, 2, 3, 4, 6, 6, 7])\n",
    "\n",
    "print(b-a)\n",
    "\n",
    "print(np.linalg.norm(a - b, axis = 1))\n",
    "\n",
    "print(np.argmin(np.linalg.norm(a - b, axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "f = open('write.csv','w', newline='')\n",
    "wr = csv.writer(f)\n",
    "\n",
    "c = np.array([1, 2, 3])\n",
    "\n",
    "wr.writerow(list(a[0]) + [c[0]])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[4, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "a = dict()\n",
    "a['a'] = [1, 2, 3, 4, 5]\n",
    "a['b'] = [4, 2, 3]\n",
    "\n",
    "for i in a:\n",
    "    print(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'e' 'b']\n",
      "['d', 'a', 'e']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 중복 허용\n",
    "count = 2\n",
    "sampleList = ['a', 'b', 'c', 'd', 'e']\n",
    "print (np.random.choice(sampleList, 3, replace=False))\n",
    "\n",
    "# 중복 허용 X \n",
    "print (random.sample(sampleList, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
